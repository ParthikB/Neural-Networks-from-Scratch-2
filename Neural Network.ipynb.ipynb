{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN v1.0",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ParthikB/Neural-Networks-from-Scratch-2/blob/NN-v1.0/Neural%20Network.ipynb.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "adP37luQEykF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "##############################################\n",
        "\n",
        "def create_data():\n",
        "  height = [130, 140, 135, 125, 120]\n",
        "  weight = [35 ,  30,  30,  35,  35]\n",
        "  y      = [  0,   1,   0,   1,   1]    \n",
        "  BMI    = [\"Healthy\", \"Unhealthy\", \"Healthy\", \"Unhealthy\", \"Unhealthy\"]\n",
        "\n",
        "  return np.array([height, weight]), np.array(y).reshape(1, -1)\n",
        "  \n",
        "\n",
        "def sigmoid(z):\n",
        "  A =  1 / (1 + np.exp(-z))\n",
        "  cache = z\n",
        "  return A, cache\n",
        "\n",
        "\n",
        "def sigmoid_derivative(dA, cache):\n",
        "  # Derivative of sigmoid: f'(x) = f(x) * (1 - f(x))\n",
        "    Z = cache\n",
        "    s = 1 / (1 + np.exp(-Z))\n",
        "    dZ = dA * s * (1 - s)\n",
        "    return dZ\n",
        "\n",
        "\n",
        "def network_structure(X, neurons_in_hidden_layer, y):\n",
        "  n_x = X.shape[0]\n",
        "  n_h = neurons_in_hidden_layer\n",
        "  n_y = y.shape[0]\n",
        "  return (n_x, n_h, n_y)\n",
        "  \n",
        "\n",
        "def initialize_random_parameters(layer_dims, X):\n",
        "    parameters = {}\n",
        "    parameters[\"W1\"] = np.random.randn(layer_dims[0], X.shape[0]) * 0.01\n",
        "    parameters[\"b1\"] = np.zeros((layer_dims[0], 1))\n",
        "    for i in range(1, len(layer_dims)):\n",
        "        parameters[\"W\" + str(i+1)] = np.random.randn(layer_dims[i], layer_dims[i-1]) * 0.01\n",
        "        parameters[\"b\" + str(i+1)] = np.zeros((layer_dims[i], 1))\n",
        "\n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eEdCM-3KQ1Aq",
        "colab_type": "text"
      },
      "source": [
        "### Basic Neuron Working"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dq6Car8gb_ma",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ca1f5368-6fea-49ae-8026-7a09864899ab"
      },
      "source": [
        "# Defining main Class.\n",
        "class Neuron:\n",
        "  \n",
        "  def __init__(self, weights, bias):\n",
        "    self.weights = weights\n",
        "    self.bias   = bias\n",
        "    \n",
        "    \n",
        "  def feedforward(self, X):\n",
        "    # Weights inputs, add bias and then use activation function\n",
        "    math_e_magic = np.dot(self.weights, X) + self.bias\n",
        "    output = sigmoid(math_e_magic)\n",
        "    \n",
        "    return output\n",
        "    \n",
        "    \n",
        "# Testing\n",
        "X = [2, 3]\n",
        "weights, bias = [2, 3], [4]\n",
        "\n",
        "# > Defining the Class\n",
        "neuron = Neuron(weights, bias)\n",
        "\n",
        "# > Feedforwarding\n",
        "print(neuron.feedforward(X))\n",
        "# Result --> array([0.99999996])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.99999996]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STSi9WjjQ6dF",
        "colab_type": "text"
      },
      "source": [
        "### Neural Network Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tb2y30xcesil",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class NeuralNetwork:\n",
        "  '''\n",
        "      A Neural Network with:\n",
        "        - 2 inputs\n",
        "        - a hidden layer with 2 Neurons\n",
        "        - an output layer with 1 Neuron\n",
        "        \n",
        "  '''    \n",
        "    \n",
        "#   def feedforward(self, X, parameters):\n",
        "#     W1, b1, W2, b2 = parameters\n",
        "    \n",
        "#     z1 = np.dot(W1, X) + b1\n",
        "#     a1 = sigmoid(z1)\n",
        "    \n",
        "#     z2 = np.dot(W2, a1) + b2\n",
        "#     a2 = sigmoid(z2)\n",
        "    \n",
        "#     y_hat = a2\n",
        "#     return y_hat\n",
        "\n",
        "\n",
        "  def feedforward(self, X, parameters):\n",
        "\n",
        "    def linear_forward(A, W, b):\n",
        "      Z = np.dot(W, A) + b\n",
        "      linear_cache = (A, W, b)\n",
        "\n",
        "      A, activation_cache = sigmoid(Z)\n",
        "\n",
        "      cache = (linear_cache, activation_cache)\n",
        "      return A, cache\n",
        "\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) // 2\n",
        "\n",
        "    for i in range(1, L+1):\n",
        "      A, cache = linear_forward(A, parameters[\"W\"+str(i)], parameters[\"b\"+str(i)])\n",
        "      caches.append(cache)\n",
        "\n",
        "    return A, caches\n",
        "\n",
        "  \n",
        "  def cost(self, yhat, y):\n",
        "    m = y.shape[1]\n",
        "    cost = -np.sum(y * np.log(yhat) + (1-y) * np.log(1-yhat)) / m\n",
        "    return cost\n",
        "  \n",
        "  \n",
        "  def backward_propagation(self, yhat, y, caches):\n",
        "\n",
        "    def linear_backward(dA, cache):\n",
        "      linear_cache, activation_cache = cache\n",
        "      A, W, b = linear_cache\n",
        "\n",
        "      dZ = sigmoid_derivative(dA, activation_cache)  \n",
        "      A_prev = A\n",
        "      m = A_prev.shape[1]\n",
        "\n",
        "      dW = np.dot(dZ, A_prev.T) / m\n",
        "      db = np.sum(dZ, axis=1, keepdims=True) / m\n",
        "      dA_prev = np.dot(W.T, dZ)\n",
        "\n",
        "      return dA_prev, dW, db\n",
        "\n",
        "\n",
        "    L = len(caches)\n",
        "    grads = {}\n",
        "    y = y.reshape(yhat.shape)\n",
        "    dyhat = -(np.divide(y, yhat) - np.divide(1-y, 1-yhat))\n",
        "    grads[\"dA\" + str(L)] = dyhat\n",
        "    \n",
        "    for l in range(L)[::-1]:\n",
        "        current_cache = caches[l]\n",
        "        grads[\"dA\" + str(l)], grads[\"dW\" + str(l+1)], grads[\"db\" + str(l+1)] = linear_backward(grads[\"dA\" + str(l+1)], current_cache)\n",
        "\n",
        "    return grads\n",
        "  \n",
        "  \n",
        "  def gradient_descent(parameters, grads):\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ln0yjG87Owma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, y = create_data()\n",
        "structure = network_structure(X, 2, y)\n",
        "paras = parameters(structure)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItLKf51XgaTk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn = NeuralNetwork()\n",
        "\n",
        "yhat, caches = nn.feedforward(X, paras)\n",
        "cost = nn.cost(yhat, y)\n",
        "grads = nn.backward_propagation(yhat, y, caches)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKT1HcMwRHMm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}